{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ETAP 1"
      ],
      "metadata": {
        "id": "V6C3smbU1C_X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CW8-bRiS7vjx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import resnet18"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ustawienia\n",
        "batch_size = 64\n",
        "num_classes = 50\n",
        "learning_rate = 0.01\n",
        "epochs = 25"
      ],
      "metadata": {
        "id": "utmJsJrEn2Ha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Przygotowanie danych\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siScrMOBn4m9",
        "outputId": "137abff4-865f-429e-fa87-7096acc8a334"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Wybieram pierwsze 50 klas\n",
        "trainset.targets = torch.tensor(trainset.targets)\n",
        "train_indices = torch.where(trainset.targets < num_classes)[0]\n",
        "trainloader = torch.utils.data.DataLoader(torch.utils.data.Subset(trainset, train_indices), batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "IHp6zxvyn7or"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wybieram pierwsze 50 klas\n",
        "trainset.targets = torch.tensor(trainset.targets)\n",
        "train_indices = torch.where(trainset.targets < num_classes)[0]\n",
        "trainloader = torch.utils.data.DataLoader(torch.utils.data.Subset(trainset, train_indices), batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdpDHMPXoEnj",
        "outputId": "c25dcadd-26bd-4332-dc99-66d1520c0f0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-67d21b9333dd>:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  trainset.targets = torch.tensor(trainset.targets)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testset.targets = torch.tensor(testset.targets)\n",
        "test_indices = torch.where(testset.targets < num_classes)[0]\n",
        "testloader = torch.utils.data.DataLoader(torch.utils.data.Subset(testset, test_indices), batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "vu2WbCFO0dsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definicja modelu\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CNN, self).__init__()\n",
        "        self.feature_extractor = resnet18(pretrained=False)\n",
        "        self.feature_extractor.fc = nn.Linear(self.feature_extractor.fc.in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.feature_extractor(x)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = CNN(num_classes=num_classes).to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vohQsqg20hQl",
        "outputId": "b69c740c-f5ef-4dbd-b909-369a4b106d05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Funkcja kosztu i optymalizator\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "SoWIqzb60kUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trenowanie modelu\n",
        "def train_model():\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in trainloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(trainloader)}\")\n"
      ],
      "metadata": {
        "id": "x-x06fhq0nSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testowanie modelu\n",
        "def test_model():\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in testloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    print(f\"Accuracy: {100 * correct / total}%\")\n",
        "\n",
        "train_model()\n",
        "test_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNPlrTI80qC0",
        "outputId": "4e0b8132-ec41-4488-8164-2900a1baefaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25, Loss: 3.66040355531151\n",
            "Epoch 2/25, Loss: 3.118846065857831\n",
            "Epoch 3/25, Loss: 2.7513762648453186\n",
            "Epoch 4/25, Loss: 2.392432375942045\n",
            "Epoch 5/25, Loss: 2.0411629234738364\n",
            "Epoch 6/25, Loss: 1.7323831908233331\n",
            "Epoch 7/25, Loss: 1.437927409511088\n",
            "Epoch 8/25, Loss: 1.154983504196567\n",
            "Epoch 9/25, Loss: 0.8617766401194551\n",
            "Epoch 10/25, Loss: 0.5663968526265201\n",
            "Epoch 11/25, Loss: 0.3319317531555205\n",
            "Epoch 12/25, Loss: 0.20695663510304887\n",
            "Epoch 13/25, Loss: 0.15731506622241587\n",
            "Epoch 14/25, Loss: 0.13676562901500547\n",
            "Epoch 15/25, Loss: 0.13056816223561002\n",
            "Epoch 16/25, Loss: 0.11831398557661973\n",
            "Epoch 17/25, Loss: 0.09466334768687673\n",
            "Epoch 18/25, Loss: 0.08481829334050417\n",
            "Epoch 19/25, Loss: 0.07685886888676668\n",
            "Epoch 20/25, Loss: 0.10185854148615123\n",
            "Epoch 21/25, Loss: 0.0748552166299461\n",
            "Epoch 22/25, Loss: 0.07645841454610686\n",
            "Epoch 23/25, Loss: 0.08880160553881046\n",
            "Epoch 24/25, Loss: 0.05287027526039468\n",
            "Epoch 25/25, Loss: 0.05417009436315797\n",
            "Accuracy: 52.94%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"model.pth\")"
      ],
      "metadata": {
        "id": "lioNnddC-3RW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Użycie wytrenowanego modelu jako ekstraktora cech\n",
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "        self.feature_extractor = nn.Sequential(*list(model.feature_extractor.children())[:-1])  # Bez ostatniej warstwy FC\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.feature_extractor(x)\n",
        "        return x.view(x.size(0), -1)\n",
        "\n",
        "# Podział zbioru na A i B\n",
        "half_classes = num_classes // 2\n",
        "trainset_A_indices = np.where(np.array(trainset.targets) < half_classes)[0]\n",
        "trainset_B_indices = np.where(np.array(trainset.targets) >= half_classes)[0]\n",
        "\n",
        "trainloader_A = torch.utils.data.DataLoader(torch.utils.data.Subset(trainset, trainset_A_indices), batch_size=batch_size, shuffle=True)\n",
        "trainloader_B = torch.utils.data.DataLoader(torch.utils.data.Subset(trainset, trainset_B_indices), batch_size=batch_size, shuffle=True)\n",
        "\n",
        "testset_B_indices = np.where(np.array(testset.targets) >= half_classes)[0]\n",
        "testloader_B = torch.utils.data.DataLoader(torch.utils.data.Subset(testset, testset_B_indices), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Trenujemy ekstraktor cech tylko na zbiorze A\n",
        "model.eval()\n",
        "feature_extractor_A = FeatureExtractor(model).to(device)\n",
        "\n",
        "def extract_features(loader, feature_extractor):\n",
        "    features, labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            feat = feature_extractor(inputs).cpu().numpy()\n",
        "            features.append(feat)\n",
        "            labels.append(targets.cpu().numpy())\n",
        "    return np.vstack(features), np.hstack(labels)\n",
        "\n",
        "train_features_A, train_labels_A = extract_features(trainloader_A, feature_extractor_A)\n",
        "test_features_A, test_labels_A = extract_features(testloader, feature_extractor_A)\n",
        "\n",
        "# Testowanie dla różnych wielkości podzbiorów na zbiorze A\n",
        "def sample_subset(features, labels, samples_per_class):\n",
        "    sampled_features, sampled_labels = [], []\n",
        "    unique_classes = np.unique(labels)\n",
        "    for cls in unique_classes:\n",
        "        indices = np.where(labels == cls)[0]\n",
        "        chosen_indices = np.random.choice(indices, min(samples_per_class, len(indices)), replace=False)\n",
        "        sampled_features.append(features[chosen_indices])\n",
        "        sampled_labels.append(labels[chosen_indices])\n",
        "    return np.vstack(sampled_features), np.hstack(sampled_labels)\n",
        "\n",
        "def knn_classification(train_features, train_labels, test_features, test_labels, k=5):\n",
        "    knn = KNeighborsClassifier(n_neighbors=k, metric='cosine')\n",
        "    knn.fit(train_features, train_labels)\n",
        "    predictions = knn.predict(test_features)\n",
        "    accuracy = np.mean(predictions == test_labels)\n",
        "    print(f'Accuracy dla k={k}: {accuracy * 100:.2f}%')\n",
        "\n",
        "for subset_size in [1, 5, 10]:\n",
        "    sampled_train_features_A, sampled_train_labels_A = sample_subset(train_features_A, train_labels_A, subset_size)\n",
        "    print(f'Czesc A: Klasyfikacja dla podzbiorow o rozmiarze {subset_size}')\n",
        "    knn_classification(sampled_train_features_A, sampled_train_labels_A, test_features_A, test_labels_A, k=5)\n",
        "\n",
        "\n",
        "train_features_B, train_labels_B = extract_features(trainloader_B, feature_extractor_A)\n",
        "test_features_B, test_labels_B = extract_features(testloader_B, feature_extractor_A)\n",
        "\n",
        "# Klasyfikacja KNN na zbiorze B\n",
        "for subset_size in [1, 5, 10]:\n",
        "    sampled_train_features_B, sampled_train_labels_B = sample_subset(train_features_B, train_labels_B, subset_size)\n",
        "    print(f'Czesc B: Klasyfikacja dla podzbiorow o rozmiarze {subset_size}')\n",
        "    knn_classification(sampled_train_features_B, sampled_train_labels_B, test_features_B, test_labels_B, k=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CN1Qjn4F0wLc",
        "outputId": "928886ed-1a75-4eee-a83a-a8170da6fffb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Czesc A: Klasyfikacja dla podzbiorow o rozmiarze 1\n",
            "Accuracy dla k=5: 7.60%\n",
            "Czesc A: Klasyfikacja dla podzbiorow o rozmiarze 5\n",
            "Accuracy dla k=5: 24.34%\n",
            "Czesc A: Klasyfikacja dla podzbiorow o rozmiarze 10\n",
            "Accuracy dla k=5: 27.14%\n",
            "Czesc B: Klasyfikacja dla podzbiorow o rozmiarze 1\n",
            "Accuracy dla k=5: 9.84%\n",
            "Czesc B: Klasyfikacja dla podzbiorow o rozmiarze 5\n",
            "Accuracy dla k=5: 20.32%\n",
            "Czesc B: Klasyfikacja dla podzbiorow o rozmiarze 10\n",
            "Accuracy dla k=5: 24.25%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import numpy as np\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Parametry\n",
        "batch_size = 128\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_classes = 100  # CIFAR-100\n",
        "\n",
        "# Transformacje\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Wczytanie zbioru danych\n",
        "trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "testset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Podział zbioru na A i B\n",
        "half_classes = num_classes // 2\n",
        "trainset_A_indices = np.where(np.array(trainset.targets) < half_classes)[0]\n",
        "trainset_B_indices = np.where(np.array(trainset.targets) >= half_classes)[0]\n",
        "\n",
        "testset_A_indices = np.where(np.array(testset.targets) < half_classes)[0]\n",
        "testset_B_indices = np.where(np.array(testset.targets) >= half_classes)[0]\n",
        "\n",
        "trainloader_A = torch.utils.data.DataLoader(torch.utils.data.Subset(trainset, trainset_A_indices), batch_size=batch_size, shuffle=True)\n",
        "trainloader_B = torch.utils.data.DataLoader(torch.utils.data.Subset(trainset, trainset_B_indices), batch_size=batch_size, shuffle=True)\n",
        "\n",
        "testloader_A = torch.utils.data.DataLoader(torch.utils.data.Subset(testset, testset_A_indices), batch_size=batch_size, shuffle=False)\n",
        "testloader_B = torch.utils.data.DataLoader(torch.utils.data.Subset(testset, testset_B_indices), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Definicja ekstraktora cech\n",
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "        self.feature_extractor = nn.Sequential(*list(model.feature_extractor.children())[:-1])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.feature_extractor(x)\n",
        "        return x.view(x.size(0), -1)\n",
        "\n",
        "# Wczytanie modelu\n",
        "model = CNN(num_classes=50)\n",
        "model.load_state_dict(torch.load(\"model.pth\"))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "feature_extractor_A = FeatureExtractor(model).to(device)\n",
        "\n",
        "def extract_features(loader, feature_extractor):\n",
        "    features, labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            feat = feature_extractor(inputs).cpu().numpy()\n",
        "            features.append(feat)\n",
        "            labels.append(targets.cpu().numpy())\n",
        "    return np.vstack(features), np.hstack(labels)\n",
        "\n",
        "train_features_A, train_labels_A = extract_features(trainloader_A, feature_extractor_A)\n",
        "test_features_A, test_labels_A = extract_features(testloader_A, feature_extractor_A)\n",
        "\n",
        "train_features_B, train_labels_B = extract_features(trainloader_B, feature_extractor_A)\n",
        "test_features_B, test_labels_B = extract_features(testloader_B, feature_extractor_A)\n",
        "\n",
        "def sample_subset(features, labels, samples_per_class):\n",
        "    sampled_features, sampled_labels = [], []\n",
        "    unique_classes = np.unique(labels)\n",
        "    for cls in unique_classes:\n",
        "        indices = np.where(labels == cls)[0]\n",
        "        chosen_indices = np.random.choice(indices, min(samples_per_class, len(indices)), replace=False)\n",
        "        sampled_features.append(features[chosen_indices])\n",
        "        sampled_labels.append(labels[chosen_indices])\n",
        "    return np.vstack(sampled_features), np.hstack(sampled_labels)\n",
        "\n",
        "def cosine_similarity_classification(train_features, train_labels, test_features, test_labels):\n",
        "    similarities = cosine_similarity(test_features, train_features)\n",
        "    predicted_labels = train_labels[np.argmax(similarities, axis=1)]\n",
        "    accuracy = np.mean(predicted_labels == test_labels)\n",
        "    print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Klasyfikacja dla A\n",
        "for subset_size in [1, 5, 10]:\n",
        "    sampled_train_features_A, sampled_train_labels_A = sample_subset(train_features_A, train_labels_A, subset_size)\n",
        "    print(f'Czesc A: Klasyfikacja dla podzbiorow o rozmiarze {subset_size}')\n",
        "    cosine_similarity_classification(sampled_train_features_A, sampled_train_labels_A, test_features_A, test_labels_A)\n",
        "\n",
        "# Klasyfikacja dla B\n",
        "for subset_size in [1, 5, 10]:\n",
        "    sampled_train_features_B, sampled_train_labels_B = sample_subset(train_features_B, train_labels_B, subset_size)\n",
        "    print(f'Czesc B: Klasyfikacja dla podzbiorow o rozmiarze {subset_size}')\n",
        "    cosine_similarity_classification(sampled_train_features_B, sampled_train_labels_B, test_features_B, test_labels_B)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJUkK31E0wFE",
        "outputId": "fd50d6c0-2aa2-46e2-ffb2-618473aa5217"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-fa3d3ff110af>:52: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"model.pth\"))  # Wczytujemy wytrenowane wagi\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Czesc A: Klasyfikacja dla podzbiorow o rozmiarze 1\n",
            "Accuracy: 24.68%\n",
            "Czesc A: Klasyfikacja dla podzbiorow o rozmiarze 5\n",
            "Accuracy: 35.82%\n",
            "Czesc A: Klasyfikacja dla podzbiorow o rozmiarze 10\n",
            "Accuracy: 40.68%\n",
            "Czesc B: Klasyfikacja dla podzbiorow o rozmiarze 1\n",
            "Accuracy: 12.88%\n",
            "Czesc B: Klasyfikacja dla podzbiorow o rozmiarze 5\n",
            "Accuracy: 20.12%\n",
            "Czesc B: Klasyfikacja dla podzbiorow o rozmiarze 10\n",
            "Accuracy: 21.16%\n"
          ]
        }
      ]
    }
  ]
}